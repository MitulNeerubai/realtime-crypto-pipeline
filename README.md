# Real-Time Cryptocurrency Data Engineering Pipeline

 
### Project Overview

This project demonstrates a complete **end-to-end data engineering pipeline** for ingesting, processing, and analyzing **real-time cryptocurrency market data** using modern cloud and big data tools.

The pipeline simulates a real-world production setup where **live data** is streamed using **Apache Kafka**, stored in **Amazon S3**, transformed using **AWS Glue with PySpark**, and queried using **Amazon Athena** for analytics.

The goal of this project is to showcase **core data engineering concepts**, hands-on cloud skills, and practical **ETL workflows** that are commonly used in industry.


### Architecture Overview

#### High-level flow:

#### 1. Data Source
* Cryptocurrency market data is fetched from the CoinCap API.

#### 2. Streaming Ingestion (Apache Kafka)
* An Amazon EC2 instance is created to host the Kafka broker.
* The EC2 instance is accessed using SSH from the local machine.
* Apache Kafka is installed and configured on the EC2 instance.
* Kafka services are started manually on the EC2 host.
* A Kafka Producer publishes live cryptocurrency data to Kafka topics.
* A Kafka Consumer subscribes to these topics and reads the streaming data in real time.

#### 3. Raw Storage (Amazon S3 ‚Äì Raw Zone)
* Kafka consumer writes raw JSON data into Amazon S3
* This represents the raw / landing layer of the data lake

#### 4. ETL Processing (AWS Glue ‚Äì PySpark)
* AWS Glue reads raw JSON files from S3
* Applies schema enforcement, data cleaning, and transformations
* Converts JSON into Parquet format
* Writes processed data to a curated S3 path

#### 5. Metadata Management (AWS Glue Crawler)
* Crawlers scan curated data
* Create tables in AWS Glue Data Catalog

#### 6. Analytics & Querying (Amazon Athena)
* Athena queries curated Parquet data directly from S3
* Enables fast, serverless SQL analytics

### Technologies Used
* Apache Kafka ‚Äì Real-time data streaming
* Python ‚Äì API ingestion and Kafka producers/consumers
* Amazon EC2 ‚Äì Kafka broker hosting
* Amazon S3 ‚Äì Data lake storage (raw & curated zones)
* AWS Glue ‚Äì ETL processing using PySpark
* PySpark ‚Äì Data transformations and schema handling
* AWS Glue Data Catalog ‚Äì Metadata management
* Amazon Athena ‚Äì Serverless SQL querying

### Detailed Pipeline Steps
**1. Kafka Streaming Layer**

A Kafka Producer fetches live crypto data from the CoinCap API.

The producer publishes messages to a Kafka topic.

A Kafka Consumer subscribes to the topic and continuously reads messages.

Incoming messages are saved as raw JSON files in Amazon S3.

This simulates a real-time ingestion system commonly used in production pipelines.

2Ô∏è‚É£ Raw Data Storage (S3 ‚Äì Raw Zone)

Raw data is stored exactly as received (JSON format).

No transformations are applied at this stage.

This ensures data traceability and allows reprocessing if needed.

Example:

s3://kafka-crypto-mitul/raw/

3Ô∏è‚É£ ETL Processing with AWS Glue (PySpark)

An AWS Glue job performs the following:

Reads raw JSON files from S3

Applies an explicit schema using PySpark

Handles type casting and null values

Flattens nested structures (using explode() where needed)

Converts JSON data into columnar Parquet format

Writes cleaned data into the curated S3 path

Why Parquet?

Faster query performance

Lower storage cost

Optimized for analytics

Example curated path:

s3://kafka-crypto-mitul/curated/

4Ô∏è‚É£ Metadata Creation with Glue Crawler

Glue Crawlers scan the curated S3 folder

Automatically infer schema and partitions

Create tables in AWS Glue Data Catalog

Makes data discoverable by Athena

5Ô∏è‚É£ Analytics with Amazon Athena

Athena queries the curated Parquet data directly from S3

No database or servers to manage

SQL queries return analytics-ready results

Example query:

SELECT symbol, price_usd, change_percent_24hr
FROM realtime_crypto_kafka.kafka_crypto_mitul
ORDER BY price_usd DESC
LIMIT 10;

üéØ Key Data Engineering Concepts Demonstrated

Real-time streaming with Kafka

Event-driven ingestion

Data lake architecture (raw vs curated zones)

ETL using PySpark

Schema enforcement and evolution

JSON to Parquet conversion

Partition-aware querying

Serverless analytics with Athena

Cloud-native data engineering on AWS

üß† What This Project Shows Interviewers

You understand how real data pipelines are built

You know when to use Kafka vs batch processing

You can write PySpark ETL jobs

You understand S3 + Glue + Athena architecture

You know why Parquet is used

You can explain raw ‚Üí curated ‚Üí analytics layers

üöÄ Future Improvements (Optional)

Add Kafka Connect for S3 sink

Add data quality checks in Glue

Automate Glue jobs using Airflow

Add dashboards using Amazon QuickSight

Implement partitioning by date/hour

üìå Conclusion

This project replicates a real-world data engineering workflow using industry-standard tools.
It demonstrates how streaming data can be ingested, transformed, stored efficiently, and queried at scale in a cloud-native environment.
