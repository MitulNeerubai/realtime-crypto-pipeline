{
	"jobConfig": {
		"name": "crypto_raw_to_curated_etl",
		"description": "",
		"role": "arn:aws:iam::695544810685:role/glue-admin-role",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "crypto_raw_to_curated_etl.py",
		"scriptLocation": "s3://aws-glue-assets-695544810685-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-12-22T23:24:58.177Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-695544810685-us-east-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-695544810685-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"dag": {},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "from awsglue.context import GlueContext\r\nfrom pyspark.context import SparkContext\r\nfrom pyspark.sql.functions import col, to_timestamp, year, month, dayofmonth\r\n\r\n# Initialize Spark\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\n\r\n# ================================\r\n# 1. READ RAW JSON FILES\r\n# ================================\r\n\r\nraw_df = spark.read \\\r\n    .option(\"multiline\", \"true\") \\\r\n    .json(\"s3://kafka-crypto-mitul/kafka-data/\")\r\n\r\n# ================================\r\n# 2. SELECT & CLEAN COLUMNS\r\n# ================================\r\nclean_df = raw_df.select(\r\n    to_timestamp(col(\"timestamp\")).alias(\"event_time\"),\r\n    col(\"id\"),\r\n    col(\"symbol\"),\r\n    col(\"name\"),\r\n    col(\"rank\"),\r\n    col(\"priceusd\").alias(\"price_usd\"),\r\n    col(\"changepercent24hr\").alias(\"change_percent_24hr\"),\r\n    col(\"volumeusd24hr\").alias(\"volume_usd_24hr\"),\r\n    col(\"marketcapusd\").alias(\"market_cap_usd\"),\r\n    col(\"supply\"),\r\n    col(\"logo\")\r\n)\r\n\r\n# ================================\r\n# 3. ADD PARTITIONS\r\n# ================================\r\nfinal_df = clean_df \\\r\n    .withColumn(\"year\", year(col(\"event_time\"))) \\\r\n    .withColumn(\"month\", month(col(\"event_time\"))) \\\r\n    .withColumn(\"day\", dayofmonth(col(\"event_time\")))\r\n\r\n# ================================\r\n# 4. WRITE CURATED PARQUET\r\n# ================================\r\nfinal_df.write \\\r\n    .mode(\"append\") \\\r\n    .partitionBy(\"year\", \"month\", \"day\") \\\r\n    .parquet(\"s3://kafka-crypto-mitul/crypto-kafka-clean/\")\r\n"
}